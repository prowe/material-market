Me and my son, we’re talking about a game idea over the weekend in this game players would build small economies and trade with each other to simulate a supply chain for that game requires a way to trade items and materials like copper and wheat Cetera without both players, having to be online at the same time This is affectively a commodities market. Let’s see if we can build such a Market on a WS

One big challenge to building a market like this is that it requires the ability to know what the best price is for a given item. Players will post items for sale at a certain price and asking price, and other players will buy a quantity of items and they will purchase those from the player that’s offering the best price is a concurrency problem at scale, because while adding a bunch of records to a relational elevation, and simply clearing up, the lowest value for a given item, is simple enough in concept that results in lots of walks being taken, and ask intention on that table as a bunch of concurrent users try to buy the same item that is being sold for the lowest price in a given time

What we need is a way to take all of the items that are for sale and keep them in a sorted list and then just throttle the buyers requests such that two buyers are not trying to buy the same item at the same time this can be truly solved, basically running one instance of some server that works Buyer requests sequentially one after another however, for obvious reasons, this won’t steal very well. Instead, we could take advantage of the natural partitioning of the buyer that’s trying to buy say copper doesn’t overlap with a buyer that’s trying to buy wheat.

Both kinesis and Taska are queuing systems that provide a guarantee that for given partition key, no two instance of a consumer will consume that scene key at the same time AWS lambda configuration for consuming from a kinesis stream, actually provides a parameter that explicitly controls this attribute and by default it’s up to one so we can leverage this property to use kinesis to partition all buyer requests by material and then serve with us use a serverless lambda function to consume those events and dispatch the goods for sale to the appropriate buyers

We have two data pipelines to build a solution one for the buyers, and one for the sellers what start with the seller pipeline as it slightly simpler. A client that is trying to post materials for sale. What is your post request to her URL containing the material ID the quantity and the price per unit they’re willing to receive this item will be right into a Llamada Niel Llamada function you’re off that lambda well then land this record into Dynamo DB table importantly, is Dynamo DB table needs to use a specific key strategy in order to ensure that we can easily quarry up material best so use the the the hash key will be the material and the range key will start with a padded price however, two clients could offer to sell the same material for the same price so that doesn’t result in the key so will just concatenate your random UU ID on the end of this to make it unique.

The fire pipeline is a little more complicated like the seller. It starts the post request to particular URL containing the material that should be purchased. The quality that has to purchased in the price the buyer is willing to pay. This URL in again be a lambda URL for simplicity revoke the lambda build the payload and lands at another Dynamo DB table represents buyer purchases this is necessary because we need a queue up the purchases and they need to be someplace actually store the status to the buyers Corey up how their orders are looking we also need to put these messages on two cases datastream so they can be repartition based on the material that’s been purchased. Well we could just use the Kenisha CPI simply put a message on the queue that introduces a transactional problem or record clear the table and then we could feel to put on the street or conversely, if we put on the stream first we could feel Tim do you throw in the diorama? A good solution is for Mr. put the stream on the other side of dynamo by listening to the Dynamo DB stream, we can get an event every time a record is Punta de Medici. We can subscribe to these events and then put the message onto kinesis, however, writing even something as simple as a William that I simply received a message to turn around and just landed on another stream. It’s kind of annoying by bill development.

The recently introduced AWS event bridge takes service is purpose, built for this kind of just plug into PE type situation